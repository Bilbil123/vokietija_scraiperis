{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import sleep\n",
    "\n",
    "# Sukuriame naršyklę su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "nustatymai.add_argument('--log-level=3')  # Slopina nereikalingus pranešimus\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "pagrindinis_url = \"https://www.trustedshops.de/shops/karneval_kostume/\"\n",
    "driver.get(pagrindinis_url)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "urls = []  # Šiame sąraše kaupsime visus URL\n",
    "puslapis = 1\n",
    "\n",
    "while True:\n",
    "    print(f\"[INFO] Scrapinamas {puslapis}-as puslapis...\")\n",
    "\n",
    "    # Surandame visus produktų blokus\n",
    "    try:\n",
    "        produktai = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".ShopResultItemstyles__ResultItem-sc-3gooul-0.dXCtjT\")))\n",
    "    except:\n",
    "        print(\"[ERROR] Produktų nerasta! Tikriname, ar puslapis tikrai kraunasi...\")\n",
    "        break\n",
    "\n",
    "    print(f\"[INFO] Rasti {len(produktai)} produktai šiame puslapyje.\")\n",
    "\n",
    "    # Pereiname per kiekvieną produktą ir išsaugome nuorodą\n",
    "    for produktas in produktai:\n",
    "        url = produktas.get_attribute(\"href\")\n",
    "        if url and url not in urls:  # Apsauga nuo dublikatų ir tuščių URL\n",
    "            urls.append(url)\n",
    "\n",
    "    # Saugome dabartinį URL, kad patikrintume, ar jis keičiasi\n",
    "    senas_url = driver.current_url\n",
    "\n",
    "    try:\n",
    "        # Tikriname, ar yra kitas puslapis\n",
    "        print(\"[INFO] Tikriname, ar yra kitas puslapis...\")\n",
    "        kitas_puslapis = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".Linkstyles__LinkAsButton-sc-1h68u9x-1.Paginationstyles__PaginationItem-sc-1uibxtv-1.jwPbFO.gXvbNn.hide-on-small-mobile\")))\n",
    "\n",
    "        if \"disabled\" in kitas_puslapis.get_attribute(\"class\"):\n",
    "            print(\"[INFO] Paskutinis puslapis pasiektas. Išeiname iš ciklo.\")\n",
    "            break\n",
    "\n",
    "        # Spaudžiame su JavaScript\n",
    "        driver.execute_script(\"arguments[0].click();\", kitas_puslapis)\n",
    "        print(\"[INFO] Mygtukas paspaustas!\")\n",
    "\n",
    "        # Laukiame, kol URL pasikeis\n",
    "        wait.until(EC.url_changes(senas_url))  # Tikriname, ar URL tikrai pasikeitė\n",
    "\n",
    "        # Patikriname, ar URL pasikeitė\n",
    "        naujas_url = driver.current_url\n",
    "        if senas_url == naujas_url:\n",
    "            print(\"[ERROR] Paspaudus mygtuką puslapis nepasikeitė. Paskutinis puslapis pasiektas.\")\n",
    "            break\n",
    "\n",
    "        # Padidiname puslapio skaičių\n",
    "        puslapis += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[INFO] Kito puslapio mygtukas nerastas. Paskutinis puslapis pasiektas. Klaida: {e}\")\n",
    "        break\n",
    "\n",
    "# Įrašome URL į failą\n",
    "failo_pavadinimas = \"Karneval_Kostume_entdecken.txt\"\n",
    "with open(failo_pavadinimas, \"w\", encoding=\"utf-8\") as f:\n",
    "    for url in urls:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "print(f\"[INFO] Iš viso surinkta {len(urls)} URL'ų. Duomenys išsaugoti į '{failo_pavadinimas}'.\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     puslapis += 1\n",
    "\n",
    "# # **Spausdiname rezultatą**\n",
    "# print(f\"[SUCCESS] Iš viso surinkta produktų: {len(urls)}\")\n",
    "\n",
    "# # **Išsaugome į failą**\n",
    "# with open(\"produktai_urls.txt\", \"w\", encoding=\"utf-8\") as failas:\n",
    "#     for url in urls:\n",
    "#         failas.write(url + \"\\n\")\n",
    "\n",
    "# print(\"[INFO] URL sąrašas išsaugotas į 'produktai_urls.txt'!\")\n",
    "\n",
    "# driver.quit()\n",
    "\n",
    "def reiksmes_filtras(driver, selektorius) :\n",
    "    try :\n",
    "        return driver.find_element(By.CSS_SELECTOR, selektorius).text\n",
    "    except :\n",
    "        return \"\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for url in urls :\n",
    "        driver.get(url)\n",
    "\n",
    "        pavadinimas = reiksmes_filtras(driver, \"h1.name\")  \n",
    "\n",
    "pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "print(pavadinimas)\n",
    "\n",
    "        kaina = reiksmes_filtras(driver, \".price span\") + \",\" + reiksmes_filtras(driver, \".price div sup\")\n",
    "        salis = reiksmes_filtras(driver, \"ul.list:first-child .item:nth-child(1) p\")\n",
    "        prekes_zenklas = reiksmes_filtras(driver, \"ul.list:first-child .item:nth-child(2) p\")\n",
    "        gamintojas = reiksmes_filtras(driver, \"ul.list:first-child .item:nth-child(3) p\")\n",
    "        svoris = reiksmes_filtras(driver, \"ul.list:first-child .item:nth-child(4) p\")\n",
    "        print(pavadinimas, kaina, salis, prekes_zenklas, gamintojas, svoris)\n",
    "\n",
    "data.append(\";\".join([\n",
    "    pavadinimas,\n",
    "    kaina,\n",
    "    salis,\n",
    "    prekes_zenklas,\n",
    "    gamintojas,\n",
    "    svoris\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas kuris ištaukia iš kategorijos url visus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Selenium nustatymai\n",
    "options = Options()\n",
    "# NEnaudosime headless, kad būtų matoma naršyklė\n",
    "# options.add_argument(\"--headless\") \n",
    "\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"start-maximized\")  # Atidarys pilno ekrano režimu\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# Pradinė URL nuoroda\n",
    "base_url = \"https://www.trustedshops.de/shops/consulting/?page=\"\n",
    "urls = []\n",
    "max_pages = 97  # Sustos ties 217 puslapiu\n",
    "\n",
    "for current_page in range(1, max_pages + 1):\n",
    "    full_url = base_url + str(current_page)\n",
    "    driver.get(full_url)\n",
    "    print(f\"\\n🔹 [INFO] Scrapinamas {current_page}-as puslapis ({full_url})\")\n",
    "\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "        product_links = wait.until(EC.presence_of_all_elements_located(\n",
    "            (By.CSS_SELECTOR, \".ShopResultItemstyles__ResultItem-sc-3gooul-0.dXCtjT\")\n",
    "        ))\n",
    "\n",
    "        page_urls = [product.get_attribute(\"href\") for product in product_links if product.get_attribute(\"href\")]\n",
    "        print(f\"✅ [INFO] Rasti {len(page_urls)} URL'ai šiame puslapyje.\")\n",
    "\n",
    "        if page_urls:\n",
    "            print(f\"🔍 [DEBUG] Pirmas URL šiame puslapyje: {page_urls[0]}\")\n",
    "\n",
    "        before_add = len(urls)\n",
    "        urls.extend([url for url in page_urls if url not in urls])\n",
    "        after_add = len(urls)\n",
    "        print(f\"🔄 [INFO] Pridėta {after_add - before_add} naujų URL. Iš viso surinkta: {len(urls)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ [ERROR] Klaida puslapyje {current_page}: {e}\")\n",
    "\n",
    "    wait_time = random.randint(8, 12)  # Atsitiktinė pauzė tarp 8-12 sekundžių\n",
    "    print(f\"⏳ [INFO] Laukiama {wait_time} sek. prieš kitą puslapį...\\n\")\n",
    "    time.sleep(wait_time)  # Daro pertrauką, kad svetainė neblokuotų\n",
    "\n",
    "print(f\"\\n📄 [INFO] Surinkta {len(urls)} URL'ų, išsaugota į 'Consulting.txt'.\")\n",
    "\n",
    "# Išsaugome į failą\n",
    "file_name = \"Consulting.txt\"\n",
    "with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    for url in urls:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veikiantis kodas paima pavadinimą"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pavadinimas: PAYBACK\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "\n",
    "# Sukuriame naršyklę su pasirinkimais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Naršyklė veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# URL, kurį norime atidaryti\n",
    "url = \"https://www.trustedshops.de/bewertung/info_X378FA6FDE903D2783D081A33BA4F164F.html\"\n",
    "driver.get(url)\n",
    "\n",
    "# Laukiame, kad puslapis užsikrautų\n",
    "sleep(3)\n",
    "\n",
    "# Pabandykime rasti pavadinimą naudojant selektorių\n",
    "try:\n",
    "    pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    print(f\"Pavadinimas: {pavadinimas}\")\n",
    "\n",
    "    # Išsaugome pavadinimą į CSV failą\n",
    "    with open(\"pavadinimai.csv\", mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([pavadinimas])  # Įrašome pavadinimą į CSV failą\n",
    "except Exception as e:\n",
    "    print(f\"Įvyko klaida: {e}\")\n",
    "\n",
    "# Baigiame naršyklės seansą\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veikiantis kodas paima aprašymą"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aprašymas: Erleben Sie eine Welt voller Trend- und Markenprodukte. Ob etwas kleines Praktisches, oder das überraschend Außergewöhnliche – die Prämienwelt von PAYBACK hat für jeden das Passende im Angebot. Belohnen Sie sich für Ihre gesammelten Punkte im PAYBACK-Prämienshop – mit garantiert tollen Angeboten!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Sukuriame naršyklę\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone (galima išjungti debug'ui)\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Nustatykite URL\n",
    "url = \"https://www.trustedshops.de/bewertung/info_X378FA6FDE903D2783D081A33BA4F164F.html\"\n",
    "driver.get(url)\n",
    "time.sleep(3)  # Palaukiame, kol puslapis užsikraus\n",
    "\n",
    "# Paimti tekstą iš aprašymo pagal nurodytą klasę\n",
    "try:\n",
    "    aprašymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    print(f\"Aprašymas: {aprašymas}\")\n",
    "except Exception as e:\n",
    "    print(f\"Klaida: {e}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas ištraukia kontaktų pilną aprašymą kompanijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telefonas: +4980036869358\n",
      "Svetainė: http://www.rossmann-fotowelt.de\n",
      "El. paštas: fotowelt@rossmann.de\n",
      "Adresas: ORWO Net GmbH Röntgenstraße 3 06766 Bitterfeld-Wolfen Deutschland\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Pateikiame URL\n",
    "url = \"https://www.trustedshops.de/bewertung/info_X09E759C6A97426A4937F97CDD8B4F8A1.html\"\n",
    "\n",
    "# Užklausos į svetainę\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Telefono numeris\n",
    "phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "phone_number = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "# Svetainės adresas - ieškome nuorodų su 'http' arba 'https'\n",
    "website = soup.find('a', href=True)\n",
    "website_url = website['href'] if website and (website['href'].startswith('http://') or website['href'].startswith('https://')) else 'Svetainės adresas nerastas'\n",
    "\n",
    "# Rasti visus elementus su klase 'contactInfo_companyContactDetailLink__OzJ99'\n",
    "links = soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True)\n",
    "\n",
    "# Inicializuojame svetainės adresą\n",
    "website_url = 'Svetainės adresas nerastas'\n",
    "\n",
    "# Tikriname kiekvieną nuorodą\n",
    "for link in links:\n",
    "    href = link['href']\n",
    "    \n",
    "    # Jei href prasideda su 'http' arba '//', tai svetainė\n",
    "    if href.startswith('http') or href.startswith('//'):\n",
    "        website_url = href if href.startswith('http') else 'http:' + href\n",
    "        break  # Radome svetainę, daugiau nebereikia ieškoti\n",
    "\n",
    "# El. paštas\n",
    "email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "email_address = email.string if email else 'El. paštas nerastas'\n",
    "\n",
    "# Adresas\n",
    "# Surandame visus div elementus su klase 'contactInfo_companyContactDetail__d2tsS'\n",
    "address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "\n",
    "# Filtruojame, kad nebūtų telefono numerio ar svetainės\n",
    "address_lines = []\n",
    "for block in address_blocks:\n",
    "    # Tikriname, ar nėra <a> nuorodų (nes jos yra telefono ir svetainės adresai)\n",
    "    if not block.find('a'):\n",
    "        # Skaldome viduje esančius elementus, kad nebūtų prilipusių žodžių\n",
    "        parts = [el.strip() for el in block.stripped_strings]\n",
    "        address_lines.extend(parts)  # Pridedame kiekvieną dalį atskirai\n",
    "\n",
    "# Sudedame adresą su tarpais tarp eilučių\n",
    "address_full = ' '.join(address_lines).replace('\\xa0', ' ')  # \\xa0 pašalina kietuosius tarpus\n",
    "\n",
    "# Atspausdinkite ištrauktus duomenis\n",
    "print(\"Telefonas:\", phone_number)\n",
    "print(\"Svetainė:\", website_url)\n",
    "print(\"El. paštas:\", email_address)\n",
    "print(\"Adresas:\", address_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas randa svetainės adresą"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Svetainė: http://www.enjoyyourcamera.com\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Pateikiame URL\n",
    "url = \"https://www.trustedshops.de/bewertung/info_X9780E98D167B6BA34F80D50738715136.html\"\n",
    "\n",
    "# Užklausos į svetainę\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Rasti visus elementus su klase 'contactInfo_companyContactDetailLink__OzJ99'\n",
    "links = soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True)\n",
    "\n",
    "# Inicializuojame svetainės adresą\n",
    "website_url = 'Svetainės adresas nerastas'\n",
    "\n",
    "# Tikriname kiekvieną nuorodą\n",
    "for link in links:\n",
    "    href = link['href']\n",
    "    \n",
    "    # Jei href prasideda su 'http' arba '//', tai svetainė\n",
    "    if href.startswith('http') or href.startswith('//'):\n",
    "        website_url = href if href.startswith('http') else 'http:' + href\n",
    "        break  # Radome svetainę, daugiau nebereikia ieškoti\n",
    "\n",
    "# Atspausdiname tik svetainės adresą\n",
    "print(\"Svetainė:\", website_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas ištraukia adresą"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adresas: Berliner Zinnfiguren Werner Scholtz e.K. Knesebeckstr. 88 10623 Berlin Deutschland\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Pateikiame URL\n",
    "url = \"https://www.trustedshops.de/bewertung/info_XCC56B1E1AD0AF5961A229D1E45BA1F18.html\"\n",
    "\n",
    "# Užklausos į svetainę\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Surandame visus div elementus su klase 'contactInfo_companyContactDetail__d2tsS'\n",
    "address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "\n",
    "# Filtruojame, kad nebūtų telefono numerio ar svetainės\n",
    "address_lines = []\n",
    "for block in address_blocks:\n",
    "    # Tikriname, ar nėra <a> nuorodų (nes jos yra telefono ir svetainės adresai)\n",
    "    if not block.find('a'):\n",
    "        # Skaldome viduje esančius elementus, kad nebūtų prilipusių žodžių\n",
    "        parts = [el.strip() for el in block.stripped_strings]\n",
    "        address_lines.extend(parts)  # Pridedame kiekvieną dalį atskirai\n",
    "\n",
    "# Sudedame adresą su tarpais tarp eilučių\n",
    "address_full = ' '.join(address_lines).replace('\\xa0', ' ')  # \\xa0 pašalina kietuosius tarpus\n",
    "\n",
    "# Atspausdiname tik įmonės adresą\n",
    "print(\"Adresas:\", address_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veikiantis kodas, kuris iš vieno url paima visą reikiamą informaciją. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Sukuriame naršyklę su pasirinkimais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone (galima išjungti debug'ui)\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# URL, kurį norime atidaryti\n",
    "url = \"https://www.trustedshops.de/bewertung/info_XAB537DF39AABA6E7BF60C330A2C36FFD.html\"\n",
    "driver.get(url)\n",
    "time.sleep(3)  # Laukiame, kol puslapis užsikraus\n",
    "\n",
    "# Pavadinimas\n",
    "try:\n",
    "    pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "except Exception as e:\n",
    "    pavadinimas = 'Pavadinimas nerastas'\n",
    "    print(f\"Klaida renkant pavadinimą: {e}\")\n",
    "\n",
    "# Aprašymas\n",
    "try:\n",
    "    aprašymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "except Exception as e:\n",
    "    aprašymas = 'Aprašymas nerastas'\n",
    "    print(f\"Klaida renkant aprašymą: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Užklausos į svetainę\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Telefono numeris\n",
    "phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "phone_number = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "# Svetainės adresas\n",
    "website = soup.find('a', href=True)\n",
    "website_url = website['href'] if website and (website['href'].startswith('http://') or website['href'].startswith('https://')) else 'Svetainės adresas nerastas'\n",
    "\n",
    "# Rasti visus elementus su klase 'contactInfo_companyContactDetailLink__OzJ99'\n",
    "links = soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True)\n",
    "\n",
    "# Inicializuojame svetainės adresą\n",
    "website_url = 'Svetainės adresas nerastas'\n",
    "\n",
    "# Tikriname kiekvieną nuorodą\n",
    "for link in links:\n",
    "    href = link['href']\n",
    "    if href.startswith('http') or href.startswith('//'):\n",
    "        website_url = href if href.startswith('http') else 'http:' + href\n",
    "        break  # Radome svetainę, daugiau nebereikia ieškoti\n",
    "\n",
    "# El. paštas\n",
    "email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "email_address = email.string if email else 'El. paštas nerastas'\n",
    "\n",
    "# Adresas\n",
    "address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "address_lines = []\n",
    "for block in address_blocks:\n",
    "    if not block.find('a'):  # Nefiltruojame telefono ar svetainės\n",
    "        parts = [el.strip() for el in block.stripped_strings]\n",
    "        address_lines.extend(parts)\n",
    "\n",
    "address_full = ' '.join(address_lines).replace('\\xa0', ' ')  # Pašalina kietuosius tarpus\n",
    "\n",
    "# Išsaugome informaciją į CSV failą\n",
    "with open(\"informacija.csv\", mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "    writer = csv.writer(failas)\n",
    "    writer.writerow([pavadinimas, aprašymas, phone_number, website_url, email_address, address_full])  # Įrašome visą informaciją\n",
    "\n",
    "# Atspausdinkite ištrauktus duomenis\n",
    "print(f\"Pavadinimas: {pavadinimas}\")\n",
    "print(f\"Aprašymas: {aprašymas}\")\n",
    "print(f\"Telefonas: {phone_number}\")\n",
    "print(f\"Svetainė: {website_url}\")\n",
    "print(f\"El. paštas: {email_address}\")\n",
    "print(f\"Adresas: {address_full}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testas 1 į sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sqlite3\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Sukuriame arba prisijungiame prie SQLite duomenų bazės\n",
    "conn = sqlite3.connect(\"imones_duomenys.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Sukuriame lentelę, jei ji dar neegzistuoja\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS imones (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    url TEXT,\n",
    "    pavadinimas TEXT,\n",
    "    aprasymas TEXT,\n",
    "    telefonas TEXT,\n",
    "    svetaine TEXT,\n",
    "    el_pastas TEXT,\n",
    "    adresas TEXT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# Perskaitome URL sąrašą iš failo\n",
    "with open(\"surinkti_urls.txt\", \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Sukuriame naršyklę su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per kiekvieną URL\n",
    "for url in url_sarasas:\n",
    "    print(f\"Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Palaukiame, kol puslapis užsikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # Aprašymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"Aprašymas nerastas\"\n",
    "\n",
    "    # Užklausos į svetainę su BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # Svetainė\n",
    "    links = soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True)\n",
    "    svetaine = next((link['href'] if link['href'].startswith(('http', '//')) else 'http:' + link['href'] for link in links), 'Svetainės adresas nerastas')\n",
    "\n",
    "    # El. paštas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paštas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # Išsaugome informaciją į duomenų bazę\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO imones (url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas))\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"✅ Duomenys išsaugoti: {pavadinimas}\")\n",
    "\n",
    "# Uždarome naršyklę ir duomenų bazės ryšį\n",
    "driver.quit()\n",
    "conn.close()\n",
    "\n",
    "print(\"🔄 Apdorojimas baigtas!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testas 2 į csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# CSV failo pavadinimas\n",
    "csv_failas = \"imones_duomenys.csv\"\n",
    "\n",
    "# Sukuriame CSV failą su stulpelių antraštėmis (jei jis dar neegzistuoja)\n",
    "with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "    writer = csv.writer(failas)\n",
    "    writer.writerow([\"URL\", \"Pavadinimas\", \"Aprašymas\", \"Telefonas\", \"Svetainė\", \"El. paštas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sąrašą iš failo\n",
    "with open(\"surinkti_urls.txt\", \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Sukuriame naršyklę su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per kiekvieną URL\n",
    "for url in url_sarasas:\n",
    "    print(f\"Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Palaukiame, kol puslapis užsikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # Aprašymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"Aprašymas nerastas\"\n",
    "\n",
    "    # Užklausos į svetainę su BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # Svetainė (taisytas variantas)\n",
    "    svetaine = \"Svetainės adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paštas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paštas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # Išsaugome informaciją į CSV failą\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"✅ Duomenys išsaugoti: {pavadinimas}\")\n",
    "\n",
    "# Uždarome naršyklę\n",
    "driver.quit()\n",
    "\n",
    "print(\"🔄 Apdorojimas baigtas! Duomenys išsaugoti į imones_duomenys.csv ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas su sleep 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# CSV failo pavadinimas\n",
    "csv_failas = \"imones_duomenys.csv\"\n",
    "\n",
    "# Sukuriame CSV failą su stulpelių antraštėmis (jei jis dar neegzistuoja)\n",
    "with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "    writer = csv.writer(failas)\n",
    "    writer.writerow([\"URL\", \"Pavadinimas\", \"Aprašymas\", \"Telefonas\", \"Svetainė\", \"El. paštas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sąrašą iš failo\n",
    "with open(\"surinkti_urls.txt\", \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Sukuriame naršyklę su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per kiekvieną URL\n",
    "for url in url_sarasas:\n",
    "    print(f\"Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Palaukiame, kol puslapis užsikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # Aprašymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"Aprašymas nerastas\"\n",
    "\n",
    "    # Užklausos į svetainę su BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # Svetainė (taisytas variantas)\n",
    "    svetaine = \"Svetainės adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paštas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paštas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # Išsaugome informaciją į CSV failą\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"✅ Duomenys išsaugoti: {pavadinimas}\")\n",
    "\n",
    "    # Palaukiame 5 sekundes prieš pereinant prie kito URL\n",
    "    time.sleep(30)\n",
    "\n",
    "# Uždarome naršyklę\n",
    "driver.quit()\n",
    "\n",
    "print(\"🔄 Apdorojimas baigtas! Duomenys išsaugoti į imones_duomenys.csv ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# CSV failo pavadinimas\n",
    "csv_failas = \"imones_duomenys.csv\"\n",
    "\n",
    "# Perskaitome jau surinktus URL iš CSV failo\n",
    "existing_urls = set()\n",
    "try:\n",
    "    with open(csv_failas, mode=\"r\", encoding=\"utf-8\") as failas:\n",
    "        reader = csv.reader(failas)\n",
    "        next(reader)  # Praleidžiame antraštes\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                existing_urls.add(row[0])\n",
    "except FileNotFoundError:\n",
    "    # Jei failo nėra, sukuriame jį su antraštėmis\n",
    "    with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([\"URL\", \"Pavadinimas\", \"Aprašymas\", \"Telefonas\", \"Svetainė\", \"El. paštas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sąrašą iš failo\n",
    "with open(\"surinkti_urls.txt\", \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Filtruojame tik tuos URL, kurie dar nėra apdoroti\n",
    "neapdoroti_urls = [url for url in url_sarasas if url not in existing_urls]\n",
    "\n",
    "# Sukuriame naršyklę su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per neapdorotus URL\n",
    "for url in neapdoroti_urls:\n",
    "    print(f\"Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Palaukiame, kol puslapis užsikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # Aprašymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"Aprašymas nerastas\"\n",
    "\n",
    "    # Užklausos į svetainę su BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # Svetainė\n",
    "    svetaine = \"Svetainės adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paštas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paštas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # Išsaugome informaciją į CSV failą\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"✅ Duomenys išsaugoti: {pavadinimas}\")\n",
    "\n",
    "    # Palaukiame 5 sekundes prieš pereinant prie kito URL\n",
    "    time.sleep(30)\n",
    "\n",
    "# Uždarome naršyklę\n",
    "driver.quit()\n",
    "\n",
    "print(\"🔄 Apdorojimas baigtas! Duomenys išsaugoti į imones_duomenys.csv ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas kameval kategorija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Failų pavadinimai\n",
    "txt_failas = \"Karneval_Kostume_entdecken.txt\"\n",
    "csv_failas = \"Karneval_Kostume_entdecken.csv\"\n",
    "\n",
    "# Perskaitome jau surinktus URL iš CSV failo\n",
    "existing_urls = set()\n",
    "try:\n",
    "    with open(csv_failas, mode=\"r\", encoding=\"utf-8\") as failas:\n",
    "        reader = csv.reader(failas)\n",
    "        next(reader)  # Praleidžiame antraštes\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                existing_urls.add(row[0])\n",
    "except FileNotFoundError:\n",
    "    # Jei failo nėra, sukuriame jį su antraštėmis\n",
    "    with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([\"URL\", \"Pavadinimas\", \"Aprašymas\", \"Telefonas\", \"Svetainė\", \"El. paštas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sąrašą iš failo\n",
    "with open(txt_failas, \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Filtruojame tik tuos URL, kurie dar nėra apdoroti\n",
    "neapdoroti_urls = [url for url in url_sarasas if url not in existing_urls]\n",
    "\n",
    "# Sukuriame naršyklę su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per neapdorotus URL\n",
    "for url in neapdoroti_urls:\n",
    "    print(f\"🔹 [INFO] Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(30)  # Palaukiame, kol puslapis užsikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # Aprašymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"Aprašymas nerastas\"\n",
    "\n",
    "    # Užklausos į svetainę su BeautifulSoup\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        print(f\"❌ [ERROR] Nepavyko pasiekti URL: {url}\")\n",
    "        continue  # Praleidžiame šį URL\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # Svetainė\n",
    "    svetaine = \"Svetainės adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paštas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paštas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # Išsaugome informaciją į CSV failą\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"✅ [INFO] Duomenys išsaugoti: {pavadinimas}\")\n",
    "\n",
    "    # Palaukiame 30 sekundžių prieš pereinant prie kito URL\n",
    "    time.sleep(30)\n",
    "\n",
    "# Uždarome naršyklę\n",
    "driver.quit()\n",
    "\n",
    "print(\"🔄 [INFO] Apdorojimas baigtas! Duomenys išsaugoti į Karneval_Kostume_entdecken.csv ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas auto_motorarad_zubehor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Failų pavadinimai\n",
    "txt_failas = \"Auto_Motorrad_Zubehor.txt\"\n",
    "csv_failas = \"Auto_Motorrad_Zubehor.csv\"\n",
    "\n",
    "# Perskaitome jau surinktus URL iš CSV failo\n",
    "existing_urls = set()\n",
    "try:\n",
    "    with open(csv_failas, mode=\"r\", encoding=\"utf-8\") as failas:\n",
    "        reader = csv.reader(failas)\n",
    "        next(reader)  # Praleidžiame antraštes\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                existing_urls.add(row[0])\n",
    "except FileNotFoundError:\n",
    "    # Jei failo nėra, sukuriame jį su antraštėmis\n",
    "    with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([\"URL\", \"Pavadinimas\", \"Aprašymas\", \"Telefonas\", \"Svetainė\", \"El. paštas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sąrašą iš failo\n",
    "with open(txt_failas, \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Filtruojame tik tuos URL, kurie dar nėra apdoroti\n",
    "neapdoroti_urls = [url for url in url_sarasas if url not in existing_urls]\n",
    "\n",
    "# Sukuriame naršyklę su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per neapdorotus URL\n",
    "for url in neapdoroti_urls:\n",
    "    print(f\"🔹 [INFO] Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(30)  # Palaukiame, kol puslapis užsikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # Aprašymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"Aprašymas nerastas\"\n",
    "\n",
    "    # Užklausos į svetainę su BeautifulSoup\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        print(f\"❌ [ERROR] Nepavyko pasiekti URL: {url}\")\n",
    "        continue  # Praleidžiame šį URL\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # Svetainė\n",
    "    svetaine = \"Svetainės adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paštas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paštas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # Išsaugome informaciją į CSV failą\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"✅ [INFO] Duomenys išsaugoti: {pavadinimas}\")\n",
    "\n",
    "    # Palaukiame 30 sekundžių prieš pereinant prie kito URL\n",
    "    time.sleep(30)\n",
    "\n",
    "# Uždarome naršyklę\n",
    "driver.quit()\n",
    "\n",
    "print(\"🔄 [INFO] Apdorojimas baigtas! Duomenys išsaugoti į Auto_Motorrad_Zubehor.csv ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drogerieartikel & Kosmetik kategorija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Failų pavadinimai\n",
    "txt_failas = \"Drogerieartikel_Kosmetik_entdecken.txt\"\n",
    "csv_failas = \"Drogerieartikel_Kosmetik_entdecken.csv\"\n",
    "\n",
    "# Perskaitome jau surinktus URL iš CSV failo\n",
    "existing_urls = set()\n",
    "try:\n",
    "    with open(csv_failas, mode=\"r\", encoding=\"utf-8\") as failas:\n",
    "        reader = csv.reader(failas)\n",
    "        next(reader)  # Praleidžiame antraštes\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                existing_urls.add(row[0])\n",
    "except FileNotFoundError:\n",
    "    # Jei failo nėra, sukuriame jį su antraštėmis\n",
    "    with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([\"URL\", \"Pavadinimas\", \"Aprašymas\", \"Telefonas\", \"Svetainė\", \"El. paštas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sąrašą iš failo\n",
    "with open(txt_failas, \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Filtruojame tik tuos URL, kurie dar nėra apdoroti\n",
    "neapdoroti_urls = [url for url in url_sarasas if url not in existing_urls]\n",
    "\n",
    "# Sukuriame naršyklę su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per neapdorotus URL\n",
    "for url in neapdoroti_urls:\n",
    "    print(f\"🔹 [INFO] Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(30)  # Palaukiame, kol puslapis užsikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # Aprašymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"Aprašymas nerastas\"\n",
    "\n",
    "    # Užklausos į svetainę su BeautifulSoup\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        print(f\"❌ [ERROR] Nepavyko pasiekti URL: {url}\")\n",
    "        continue  # Praleidžiame šį URL\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # Svetainė\n",
    "    svetaine = \"Svetainės adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paštas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paštas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # Išsaugome informaciją į CSV failą\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"✅ [INFO] Duomenys išsaugoti: {pavadinimas}\")\n",
    "\n",
    "    # Palaukiame 30 sekundžių prieš pereinant prie kito URL\n",
    "    time.sleep(30)\n",
    "\n",
    "# Uždarome naršyklę\n",
    "driver.quit()\n",
    "\n",
    "print(\"🔄 [INFO] Apdorojimas baigtas! Duomenys išsaugoti į Drogerieartikel_Kosmetik_entdecken.csv ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas kuris paima duomenis iš Bekleidung.txt url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Failų pavadinimai\n",
    "txt_failas = \"Bekleidung.txt\"\n",
    "csv_failas = \"Bekleidung.csv\"\n",
    "\n",
    "# Perskaitome jau surinktus URL iš CSV failo\n",
    "existing_urls = set()\n",
    "try:\n",
    "    with open(csv_failas, mode=\"r\", encoding=\"utf-8\") as failas:\n",
    "        reader = csv.reader(failas)\n",
    "        next(reader)  # Praleidžiame antraštes\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                existing_urls.add(row[0])\n",
    "except FileNotFoundError:\n",
    "    # Jei failo nėra, sukuriame jį su antraštėmis\n",
    "    with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([\"URL\", \"Pavadinimas\", \"Aprašymas\", \"Telefonas\", \"Svetainė\", \"El. paštas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sąrašą iš failo\n",
    "with open(txt_failas, \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Filtruojame tik tuos URL, kurie dar nėra apdoroti\n",
    "neapdoroti_urls = [url for url in url_sarasas if url not in existing_urls]\n",
    "\n",
    "# Sukuriame naršyklę su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per neapdorotus URL\n",
    "for url in neapdoroti_urls:\n",
    "    print(f\"🔹 [INFO] Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(30)  # Palaukiame, kol puslapis užsikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # Aprašymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"Aprašymas nerastas\"\n",
    "\n",
    "    # Užklausos į svetainę su BeautifulSoup\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        print(f\"❌ [ERROR] Nepavyko pasiekti URL: {url}\")\n",
    "        continue  # Praleidžiame šį URL\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # Svetainė\n",
    "    svetaine = \"Svetainės adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paštas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paštas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # Išsaugome informaciją į CSV failą\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"✅ [INFO] Duomenys išsaugoti: {pavadinimas}\")\n",
    "\n",
    "    # Palaukiame 30 sekundžių prieš pereinant prie kito URL\n",
    "    time.sleep(30)\n",
    "\n",
    "# Uždarome naršyklę\n",
    "driver.quit()\n",
    "\n",
    "print(\"🔄 [INFO] Apdorojimas baigtas! Duomenys išsaugoti į Drogerieartikel_Kosmetik_entdecken.csv ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer_Unterhaltungselektronik_Zubehor.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Failų pavadinimai\n",
    "txt_failas = \"Consulting.txt\"\n",
    "csv_failas = \"Consulting.csv\"\n",
    "\n",
    "# Perskaitome jau surinktus URL iš CSV failo\n",
    "existing_urls = set()\n",
    "try:\n",
    "    with open(csv_failas, mode=\"r\", encoding=\"utf-8\") as failas:\n",
    "        reader = csv.reader(failas)\n",
    "        first_line = next(reader, None)  # Praleidžiame antraštes, jei yra\n",
    "        if first_line is not None:  # Tikriname, ar failas ne tuščias\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    existing_urls.add(row[0])\n",
    "except FileNotFoundError:\n",
    "    # Jei failo nėra, sukuriame jį su antraštėmis\n",
    "    with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([\"URL\", \"Pavadinimas\", \"Aprašymas\", \"Telefonas\", \"Svetainė\", \"El. paštas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sąrašą iš failo\n",
    "with open(txt_failas, \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Filtruojame tik tuos URL, kurie dar nėra apdoroti\n",
    "neapdoroti_urls = [url for url in url_sarasas if url not in existing_urls]\n",
    "\n",
    "# Sukuriame naršyklę su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # Mažina botų aptikimą\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# HTTP antraštės, kad atrodytume kaip tikras vartotojas\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
    "\n",
    "# Keliaujame per neapdorotus URL\n",
    "for i, url in enumerate(neapdoroti_urls):\n",
    "    print(f\"🔹 [INFO] Apdorojamas URL: {url}\")\n",
    "    \n",
    "    # Pirmiausia tikriname, ar URL pasiekiamas\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        print(f\"❌ [ERROR] Nepavyko pasiekti URL: {url}\")\n",
    "        continue\n",
    "\n",
    "    # Naudojame Selenium tik jei reikia\n",
    "    driver.get(url)\n",
    "    time.sleep(30)  # Palaukiame 30 sekundžių, kol puslapis užsikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\"))\n",
    "        ).text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # Aprašymas\n",
    "    try:\n",
    "        aprasymas = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\"))\n",
    "        ).text\n",
    "    except Exception:\n",
    "        aprasymas = \"Aprašymas nerastas\"\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=lambda x: x and x.startswith('tel:'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # Svetainė\n",
    "    svetaine = \"Svetainės adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paštas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paštas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # Išsaugome informaciją į CSV failą\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"✅ [INFO] Duomenys išsaugoti: {pavadinimas}\")\n",
    "\n",
    "    # Laukiame 30 sekundžių prieš kitą URL\n",
    "    time.sleep(30)\n",
    "\n",
    "    # Perkrauname naršyklę kas 50 URL, kad išvengtume atminties problemų\n",
    "    if i % 50 == 0 and i != 0:\n",
    "        driver.quit()\n",
    "        driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Uždarome naršyklę\n",
    "driver.quit()\n",
    "\n",
    "print(\"🔄 [INFO] Apdorojimas baigtas! Duomenys išsaugoti į Consulting.csv ✅\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
