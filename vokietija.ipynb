{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import sleep\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™ su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "nustatymai.add_argument('--log-level=3')  # Slopina nereikalingus praneÅ¡imus\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "pagrindinis_url = \"https://www.trustedshops.de/shops/karneval_kostume/\"\n",
    "driver.get(pagrindinis_url)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "urls = []  # Å iame sÄ…raÅ¡e kaupsime visus URL\n",
    "puslapis = 1\n",
    "\n",
    "while True:\n",
    "    print(f\"[INFO] Scrapinamas {puslapis}-as puslapis...\")\n",
    "\n",
    "    # Surandame visus produktÅ³ blokus\n",
    "    try:\n",
    "        produktai = wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".ShopResultItemstyles__ResultItem-sc-3gooul-0.dXCtjT\")))\n",
    "    except:\n",
    "        print(\"[ERROR] ProduktÅ³ nerasta! Tikriname, ar puslapis tikrai kraunasi...\")\n",
    "        break\n",
    "\n",
    "    print(f\"[INFO] Rasti {len(produktai)} produktai Å¡iame puslapyje.\")\n",
    "\n",
    "    # Pereiname per kiekvienÄ… produktÄ… ir iÅ¡saugome nuorodÄ…\n",
    "    for produktas in produktai:\n",
    "        url = produktas.get_attribute(\"href\")\n",
    "        if url and url not in urls:  # Apsauga nuo dublikatÅ³ ir tuÅ¡ÄiÅ³ URL\n",
    "            urls.append(url)\n",
    "\n",
    "    # Saugome dabartinÄ¯ URL, kad patikrintume, ar jis keiÄiasi\n",
    "    senas_url = driver.current_url\n",
    "\n",
    "    try:\n",
    "        # Tikriname, ar yra kitas puslapis\n",
    "        print(\"[INFO] Tikriname, ar yra kitas puslapis...\")\n",
    "        kitas_puslapis = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, \".Linkstyles__LinkAsButton-sc-1h68u9x-1.Paginationstyles__PaginationItem-sc-1uibxtv-1.jwPbFO.gXvbNn.hide-on-small-mobile\")))\n",
    "\n",
    "        if \"disabled\" in kitas_puslapis.get_attribute(\"class\"):\n",
    "            print(\"[INFO] Paskutinis puslapis pasiektas. IÅ¡einame iÅ¡ ciklo.\")\n",
    "            break\n",
    "\n",
    "        # SpaudÅ¾iame su JavaScript\n",
    "        driver.execute_script(\"arguments[0].click();\", kitas_puslapis)\n",
    "        print(\"[INFO] Mygtukas paspaustas!\")\n",
    "\n",
    "        # Laukiame, kol URL pasikeis\n",
    "        wait.until(EC.url_changes(senas_url))  # Tikriname, ar URL tikrai pasikeitÄ—\n",
    "\n",
    "        # Patikriname, ar URL pasikeitÄ—\n",
    "        naujas_url = driver.current_url\n",
    "        if senas_url == naujas_url:\n",
    "            print(\"[ERROR] Paspaudus mygtukÄ… puslapis nepasikeitÄ—. Paskutinis puslapis pasiektas.\")\n",
    "            break\n",
    "\n",
    "        # Padidiname puslapio skaiÄiÅ³\n",
    "        puslapis += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[INFO] Kito puslapio mygtukas nerastas. Paskutinis puslapis pasiektas. Klaida: {e}\")\n",
    "        break\n",
    "\n",
    "# Ä®raÅ¡ome URL Ä¯ failÄ…\n",
    "failo_pavadinimas = \"Karneval_Kostume_entdecken.txt\"\n",
    "with open(failo_pavadinimas, \"w\", encoding=\"utf-8\") as f:\n",
    "    for url in urls:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "print(f\"[INFO] IÅ¡ viso surinkta {len(urls)} URL'Å³. Duomenys iÅ¡saugoti Ä¯ '{failo_pavadinimas}'.\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     puslapis += 1\n",
    "\n",
    "# # **Spausdiname rezultatÄ…**\n",
    "# print(f\"[SUCCESS] IÅ¡ viso surinkta produktÅ³: {len(urls)}\")\n",
    "\n",
    "# # **IÅ¡saugome Ä¯ failÄ…**\n",
    "# with open(\"produktai_urls.txt\", \"w\", encoding=\"utf-8\") as failas:\n",
    "#     for url in urls:\n",
    "#         failas.write(url + \"\\n\")\n",
    "\n",
    "# print(\"[INFO] URL sÄ…raÅ¡as iÅ¡saugotas Ä¯ 'produktai_urls.txt'!\")\n",
    "\n",
    "# driver.quit()\n",
    "\n",
    "def reiksmes_filtras(driver, selektorius) :\n",
    "    try :\n",
    "        return driver.find_element(By.CSS_SELECTOR, selektorius).text\n",
    "    except :\n",
    "        return \"\"\n",
    "\n",
    "data = []\n",
    "\n",
    "for url in urls :\n",
    "        driver.get(url)\n",
    "\n",
    "        pavadinimas = reiksmes_filtras(driver, \"h1.name\")  \n",
    "\n",
    "pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "print(pavadinimas)\n",
    "\n",
    "        kaina = reiksmes_filtras(driver, \".price span\") + \",\" + reiksmes_filtras(driver, \".price div sup\")\n",
    "        salis = reiksmes_filtras(driver, \"ul.list:first-child .item:nth-child(1) p\")\n",
    "        prekes_zenklas = reiksmes_filtras(driver, \"ul.list:first-child .item:nth-child(2) p\")\n",
    "        gamintojas = reiksmes_filtras(driver, \"ul.list:first-child .item:nth-child(3) p\")\n",
    "        svoris = reiksmes_filtras(driver, \"ul.list:first-child .item:nth-child(4) p\")\n",
    "        print(pavadinimas, kaina, salis, prekes_zenklas, gamintojas, svoris)\n",
    "\n",
    "data.append(\";\".join([\n",
    "    pavadinimas,\n",
    "    kaina,\n",
    "    salis,\n",
    "    prekes_zenklas,\n",
    "    gamintojas,\n",
    "    svoris\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas kuris iÅ¡taukia iÅ¡ kategorijos url visus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Selenium nustatymai\n",
    "options = Options()\n",
    "# NEnaudosime headless, kad bÅ«tÅ³ matoma narÅ¡yklÄ—\n",
    "# options.add_argument(\"--headless\") \n",
    "\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_argument(\"start-maximized\")  # Atidarys pilno ekrano reÅ¾imu\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# PradinÄ— URL nuoroda\n",
    "base_url = \"https://www.trustedshops.de/shops/consulting/?page=\"\n",
    "urls = []\n",
    "max_pages = 97  # Sustos ties 217 puslapiu\n",
    "\n",
    "for current_page in range(1, max_pages + 1):\n",
    "    full_url = base_url + str(current_page)\n",
    "    driver.get(full_url)\n",
    "    print(f\"\\nğŸ”¹ [INFO] Scrapinamas {current_page}-as puslapis ({full_url})\")\n",
    "\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 15)\n",
    "        product_links = wait.until(EC.presence_of_all_elements_located(\n",
    "            (By.CSS_SELECTOR, \".ShopResultItemstyles__ResultItem-sc-3gooul-0.dXCtjT\")\n",
    "        ))\n",
    "\n",
    "        page_urls = [product.get_attribute(\"href\") for product in product_links if product.get_attribute(\"href\")]\n",
    "        print(f\"âœ… [INFO] Rasti {len(page_urls)} URL'ai Å¡iame puslapyje.\")\n",
    "\n",
    "        if page_urls:\n",
    "            print(f\"ğŸ” [DEBUG] Pirmas URL Å¡iame puslapyje: {page_urls[0]}\")\n",
    "\n",
    "        before_add = len(urls)\n",
    "        urls.extend([url for url in page_urls if url not in urls])\n",
    "        after_add = len(urls)\n",
    "        print(f\"ğŸ”„ [INFO] PridÄ—ta {after_add - before_add} naujÅ³ URL. IÅ¡ viso surinkta: {len(urls)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ [ERROR] Klaida puslapyje {current_page}: {e}\")\n",
    "\n",
    "    wait_time = random.randint(8, 12)  # AtsitiktinÄ— pauzÄ— tarp 8-12 sekundÅ¾iÅ³\n",
    "    print(f\"â³ [INFO] Laukiama {wait_time} sek. prieÅ¡ kitÄ… puslapÄ¯...\\n\")\n",
    "    time.sleep(wait_time)  # Daro pertraukÄ…, kad svetainÄ— neblokuotÅ³\n",
    "\n",
    "print(f\"\\nğŸ“„ [INFO] Surinkta {len(urls)} URL'Å³, iÅ¡saugota Ä¯ 'Consulting.txt'.\")\n",
    "\n",
    "# IÅ¡saugome Ä¯ failÄ…\n",
    "file_name = \"Consulting.txt\"\n",
    "with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "    for url in urls:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veikiantis kodas paima pavadinimÄ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pavadinimas: PAYBACK\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from time import sleep\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™ su pasirinkimais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # NarÅ¡yklÄ— veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# URL, kurÄ¯ norime atidaryti\n",
    "url = \"https://www.trustedshops.de/bewertung/info_X378FA6FDE903D2783D081A33BA4F164F.html\"\n",
    "driver.get(url)\n",
    "\n",
    "# Laukiame, kad puslapis uÅ¾sikrautÅ³\n",
    "sleep(3)\n",
    "\n",
    "# Pabandykime rasti pavadinimÄ… naudojant selektoriÅ³\n",
    "try:\n",
    "    pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    print(f\"Pavadinimas: {pavadinimas}\")\n",
    "\n",
    "    # IÅ¡saugome pavadinimÄ… Ä¯ CSV failÄ…\n",
    "    with open(\"pavadinimai.csv\", mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([pavadinimas])  # Ä®raÅ¡ome pavadinimÄ… Ä¯ CSV failÄ…\n",
    "except Exception as e:\n",
    "    print(f\"Ä®vyko klaida: {e}\")\n",
    "\n",
    "# Baigiame narÅ¡yklÄ—s seansÄ…\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veikiantis kodas paima apraÅ¡ymÄ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ApraÅ¡ymas: Erleben Sie eine Welt voller Trend- und Markenprodukte. Ob etwas kleines Praktisches, oder das Ã¼berraschend AuÃŸergewÃ¶hnliche â€“ die PrÃ¤mienwelt von PAYBACK hat fÃ¼r jeden das Passende im Angebot. Belohnen Sie sich fÃ¼r Ihre gesammelten Punkte im PAYBACK-PrÃ¤mienshop â€“ mit garantiert tollen Angeboten!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone (galima iÅ¡jungti debug'ui)\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Nustatykite URL\n",
    "url = \"https://www.trustedshops.de/bewertung/info_X378FA6FDE903D2783D081A33BA4F164F.html\"\n",
    "driver.get(url)\n",
    "time.sleep(3)  # Palaukiame, kol puslapis uÅ¾sikraus\n",
    "\n",
    "# Paimti tekstÄ… iÅ¡ apraÅ¡ymo pagal nurodytÄ… klasÄ™\n",
    "try:\n",
    "    apraÅ¡ymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    print(f\"ApraÅ¡ymas: {apraÅ¡ymas}\")\n",
    "except Exception as e:\n",
    "    print(f\"Klaida: {e}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas iÅ¡traukia kontaktÅ³ pilnÄ… apraÅ¡ymÄ… kompanijos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telefonas: +4980036869358\n",
      "SvetainÄ—: http://www.rossmann-fotowelt.de\n",
      "El. paÅ¡tas: fotowelt@rossmann.de\n",
      "Adresas: ORWO Net GmbH RÃ¶ntgenstraÃŸe 3 06766 Bitterfeld-Wolfen Deutschland\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Pateikiame URL\n",
    "url = \"https://www.trustedshops.de/bewertung/info_X09E759C6A97426A4937F97CDD8B4F8A1.html\"\n",
    "\n",
    "# UÅ¾klausos Ä¯ svetainÄ™\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Telefono numeris\n",
    "phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "phone_number = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "# SvetainÄ—s adresas - ieÅ¡kome nuorodÅ³ su 'http' arba 'https'\n",
    "website = soup.find('a', href=True)\n",
    "website_url = website['href'] if website and (website['href'].startswith('http://') or website['href'].startswith('https://')) else 'SvetainÄ—s adresas nerastas'\n",
    "\n",
    "# Rasti visus elementus su klase 'contactInfo_companyContactDetailLink__OzJ99'\n",
    "links = soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True)\n",
    "\n",
    "# Inicializuojame svetainÄ—s adresÄ…\n",
    "website_url = 'SvetainÄ—s adresas nerastas'\n",
    "\n",
    "# Tikriname kiekvienÄ… nuorodÄ…\n",
    "for link in links:\n",
    "    href = link['href']\n",
    "    \n",
    "    # Jei href prasideda su 'http' arba '//', tai svetainÄ—\n",
    "    if href.startswith('http') or href.startswith('//'):\n",
    "        website_url = href if href.startswith('http') else 'http:' + href\n",
    "        break  # Radome svetainÄ™, daugiau nebereikia ieÅ¡koti\n",
    "\n",
    "# El. paÅ¡tas\n",
    "email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "email_address = email.string if email else 'El. paÅ¡tas nerastas'\n",
    "\n",
    "# Adresas\n",
    "# Surandame visus div elementus su klase 'contactInfo_companyContactDetail__d2tsS'\n",
    "address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "\n",
    "# Filtruojame, kad nebÅ«tÅ³ telefono numerio ar svetainÄ—s\n",
    "address_lines = []\n",
    "for block in address_blocks:\n",
    "    # Tikriname, ar nÄ—ra <a> nuorodÅ³ (nes jos yra telefono ir svetainÄ—s adresai)\n",
    "    if not block.find('a'):\n",
    "        # Skaldome viduje esanÄius elementus, kad nebÅ«tÅ³ prilipusiÅ³ Å¾odÅ¾iÅ³\n",
    "        parts = [el.strip() for el in block.stripped_strings]\n",
    "        address_lines.extend(parts)  # Pridedame kiekvienÄ… dalÄ¯ atskirai\n",
    "\n",
    "# Sudedame adresÄ… su tarpais tarp eiluÄiÅ³\n",
    "address_full = ' '.join(address_lines).replace('\\xa0', ' ')  # \\xa0 paÅ¡alina kietuosius tarpus\n",
    "\n",
    "# Atspausdinkite iÅ¡trauktus duomenis\n",
    "print(\"Telefonas:\", phone_number)\n",
    "print(\"SvetainÄ—:\", website_url)\n",
    "print(\"El. paÅ¡tas:\", email_address)\n",
    "print(\"Adresas:\", address_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas randa svetainÄ—s adresÄ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SvetainÄ—: http://www.enjoyyourcamera.com\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Pateikiame URL\n",
    "url = \"https://www.trustedshops.de/bewertung/info_X9780E98D167B6BA34F80D50738715136.html\"\n",
    "\n",
    "# UÅ¾klausos Ä¯ svetainÄ™\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Rasti visus elementus su klase 'contactInfo_companyContactDetailLink__OzJ99'\n",
    "links = soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True)\n",
    "\n",
    "# Inicializuojame svetainÄ—s adresÄ…\n",
    "website_url = 'SvetainÄ—s adresas nerastas'\n",
    "\n",
    "# Tikriname kiekvienÄ… nuorodÄ…\n",
    "for link in links:\n",
    "    href = link['href']\n",
    "    \n",
    "    # Jei href prasideda su 'http' arba '//', tai svetainÄ—\n",
    "    if href.startswith('http') or href.startswith('//'):\n",
    "        website_url = href if href.startswith('http') else 'http:' + href\n",
    "        break  # Radome svetainÄ™, daugiau nebereikia ieÅ¡koti\n",
    "\n",
    "# Atspausdiname tik svetainÄ—s adresÄ…\n",
    "print(\"SvetainÄ—:\", website_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas iÅ¡traukia adresÄ…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adresas: Berliner Zinnfiguren Werner Scholtz e.K. Knesebeckstr. 88 10623 Berlin Deutschland\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Pateikiame URL\n",
    "url = \"https://www.trustedshops.de/bewertung/info_XCC56B1E1AD0AF5961A229D1E45BA1F18.html\"\n",
    "\n",
    "# UÅ¾klausos Ä¯ svetainÄ™\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Surandame visus div elementus su klase 'contactInfo_companyContactDetail__d2tsS'\n",
    "address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "\n",
    "# Filtruojame, kad nebÅ«tÅ³ telefono numerio ar svetainÄ—s\n",
    "address_lines = []\n",
    "for block in address_blocks:\n",
    "    # Tikriname, ar nÄ—ra <a> nuorodÅ³ (nes jos yra telefono ir svetainÄ—s adresai)\n",
    "    if not block.find('a'):\n",
    "        # Skaldome viduje esanÄius elementus, kad nebÅ«tÅ³ prilipusiÅ³ Å¾odÅ¾iÅ³\n",
    "        parts = [el.strip() for el in block.stripped_strings]\n",
    "        address_lines.extend(parts)  # Pridedame kiekvienÄ… dalÄ¯ atskirai\n",
    "\n",
    "# Sudedame adresÄ… su tarpais tarp eiluÄiÅ³\n",
    "address_full = ' '.join(address_lines).replace('\\xa0', ' ')  # \\xa0 paÅ¡alina kietuosius tarpus\n",
    "\n",
    "# Atspausdiname tik Ä¯monÄ—s adresÄ…\n",
    "print(\"Adresas:\", address_full)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veikiantis kodas, kuris iÅ¡ vieno url paima visÄ… reikiamÄ… informacijÄ…. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™ su pasirinkimais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone (galima iÅ¡jungti debug'ui)\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# URL, kurÄ¯ norime atidaryti\n",
    "url = \"https://www.trustedshops.de/bewertung/info_XAB537DF39AABA6E7BF60C330A2C36FFD.html\"\n",
    "driver.get(url)\n",
    "time.sleep(3)  # Laukiame, kol puslapis uÅ¾sikraus\n",
    "\n",
    "# Pavadinimas\n",
    "try:\n",
    "    pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "except Exception as e:\n",
    "    pavadinimas = 'Pavadinimas nerastas'\n",
    "    print(f\"Klaida renkant pavadinimÄ…: {e}\")\n",
    "\n",
    "# ApraÅ¡ymas\n",
    "try:\n",
    "    apraÅ¡ymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "except Exception as e:\n",
    "    apraÅ¡ymas = 'ApraÅ¡ymas nerastas'\n",
    "    print(f\"Klaida renkant apraÅ¡ymÄ…: {e}\")\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# UÅ¾klausos Ä¯ svetainÄ™\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Telefono numeris\n",
    "phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "phone_number = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "# SvetainÄ—s adresas\n",
    "website = soup.find('a', href=True)\n",
    "website_url = website['href'] if website and (website['href'].startswith('http://') or website['href'].startswith('https://')) else 'SvetainÄ—s adresas nerastas'\n",
    "\n",
    "# Rasti visus elementus su klase 'contactInfo_companyContactDetailLink__OzJ99'\n",
    "links = soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True)\n",
    "\n",
    "# Inicializuojame svetainÄ—s adresÄ…\n",
    "website_url = 'SvetainÄ—s adresas nerastas'\n",
    "\n",
    "# Tikriname kiekvienÄ… nuorodÄ…\n",
    "for link in links:\n",
    "    href = link['href']\n",
    "    if href.startswith('http') or href.startswith('//'):\n",
    "        website_url = href if href.startswith('http') else 'http:' + href\n",
    "        break  # Radome svetainÄ™, daugiau nebereikia ieÅ¡koti\n",
    "\n",
    "# El. paÅ¡tas\n",
    "email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "email_address = email.string if email else 'El. paÅ¡tas nerastas'\n",
    "\n",
    "# Adresas\n",
    "address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "address_lines = []\n",
    "for block in address_blocks:\n",
    "    if not block.find('a'):  # Nefiltruojame telefono ar svetainÄ—s\n",
    "        parts = [el.strip() for el in block.stripped_strings]\n",
    "        address_lines.extend(parts)\n",
    "\n",
    "address_full = ' '.join(address_lines).replace('\\xa0', ' ')  # PaÅ¡alina kietuosius tarpus\n",
    "\n",
    "# IÅ¡saugome informacijÄ… Ä¯ CSV failÄ…\n",
    "with open(\"informacija.csv\", mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "    writer = csv.writer(failas)\n",
    "    writer.writerow([pavadinimas, apraÅ¡ymas, phone_number, website_url, email_address, address_full])  # Ä®raÅ¡ome visÄ… informacijÄ…\n",
    "\n",
    "# Atspausdinkite iÅ¡trauktus duomenis\n",
    "print(f\"Pavadinimas: {pavadinimas}\")\n",
    "print(f\"ApraÅ¡ymas: {apraÅ¡ymas}\")\n",
    "print(f\"Telefonas: {phone_number}\")\n",
    "print(f\"SvetainÄ—: {website_url}\")\n",
    "print(f\"El. paÅ¡tas: {email_address}\")\n",
    "print(f\"Adresas: {address_full}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testas 1 Ä¯ sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sqlite3\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# Sukuriame arba prisijungiame prie SQLite duomenÅ³ bazÄ—s\n",
    "conn = sqlite3.connect(\"imones_duomenys.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Sukuriame lentelÄ™, jei ji dar neegzistuoja\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS imones (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    url TEXT,\n",
    "    pavadinimas TEXT,\n",
    "    aprasymas TEXT,\n",
    "    telefonas TEXT,\n",
    "    svetaine TEXT,\n",
    "    el_pastas TEXT,\n",
    "    adresas TEXT\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# Perskaitome URL sÄ…raÅ¡Ä… iÅ¡ failo\n",
    "with open(\"surinkti_urls.txt\", \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™ su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per kiekvienÄ… URL\n",
    "for url in url_sarasas:\n",
    "    print(f\"Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Palaukiame, kol puslapis uÅ¾sikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # ApraÅ¡ymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"ApraÅ¡ymas nerastas\"\n",
    "\n",
    "    # UÅ¾klausos Ä¯ svetainÄ™ su BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # SvetainÄ—\n",
    "    links = soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True)\n",
    "    svetaine = next((link['href'] if link['href'].startswith(('http', '//')) else 'http:' + link['href'] for link in links), 'SvetainÄ—s adresas nerastas')\n",
    "\n",
    "    # El. paÅ¡tas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paÅ¡tas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # IÅ¡saugome informacijÄ… Ä¯ duomenÅ³ bazÄ™\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO imones (url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "    \"\"\", (url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas))\n",
    "    conn.commit()\n",
    "\n",
    "    print(f\"âœ… Duomenys iÅ¡saugoti: {pavadinimas}\")\n",
    "\n",
    "# UÅ¾darome narÅ¡yklÄ™ ir duomenÅ³ bazÄ—s ryÅ¡Ä¯\n",
    "driver.quit()\n",
    "conn.close()\n",
    "\n",
    "print(\"ğŸ”„ Apdorojimas baigtas!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testas 2 Ä¯ csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# CSV failo pavadinimas\n",
    "csv_failas = \"imones_duomenys.csv\"\n",
    "\n",
    "# Sukuriame CSV failÄ… su stulpeliÅ³ antraÅ¡tÄ—mis (jei jis dar neegzistuoja)\n",
    "with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "    writer = csv.writer(failas)\n",
    "    writer.writerow([\"URL\", \"Pavadinimas\", \"ApraÅ¡ymas\", \"Telefonas\", \"SvetainÄ—\", \"El. paÅ¡tas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sÄ…raÅ¡Ä… iÅ¡ failo\n",
    "with open(\"surinkti_urls.txt\", \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™ su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per kiekvienÄ… URL\n",
    "for url in url_sarasas:\n",
    "    print(f\"Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Palaukiame, kol puslapis uÅ¾sikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # ApraÅ¡ymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"ApraÅ¡ymas nerastas\"\n",
    "\n",
    "    # UÅ¾klausos Ä¯ svetainÄ™ su BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # SvetainÄ— (taisytas variantas)\n",
    "    svetaine = \"SvetainÄ—s adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paÅ¡tas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paÅ¡tas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # IÅ¡saugome informacijÄ… Ä¯ CSV failÄ…\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"âœ… Duomenys iÅ¡saugoti: {pavadinimas}\")\n",
    "\n",
    "# UÅ¾darome narÅ¡yklÄ™\n",
    "driver.quit()\n",
    "\n",
    "print(\"ğŸ”„ Apdorojimas baigtas! Duomenys iÅ¡saugoti Ä¯ imones_duomenys.csv âœ…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas su sleep 5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# CSV failo pavadinimas\n",
    "csv_failas = \"imones_duomenys.csv\"\n",
    "\n",
    "# Sukuriame CSV failÄ… su stulpeliÅ³ antraÅ¡tÄ—mis (jei jis dar neegzistuoja)\n",
    "with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "    writer = csv.writer(failas)\n",
    "    writer.writerow([\"URL\", \"Pavadinimas\", \"ApraÅ¡ymas\", \"Telefonas\", \"SvetainÄ—\", \"El. paÅ¡tas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sÄ…raÅ¡Ä… iÅ¡ failo\n",
    "with open(\"surinkti_urls.txt\", \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™ su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per kiekvienÄ… URL\n",
    "for url in url_sarasas:\n",
    "    print(f\"Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Palaukiame, kol puslapis uÅ¾sikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # ApraÅ¡ymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"ApraÅ¡ymas nerastas\"\n",
    "\n",
    "    # UÅ¾klausos Ä¯ svetainÄ™ su BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # SvetainÄ— (taisytas variantas)\n",
    "    svetaine = \"SvetainÄ—s adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paÅ¡tas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paÅ¡tas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # IÅ¡saugome informacijÄ… Ä¯ CSV failÄ…\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"âœ… Duomenys iÅ¡saugoti: {pavadinimas}\")\n",
    "\n",
    "    # Palaukiame 5 sekundes prieÅ¡ pereinant prie kito URL\n",
    "    time.sleep(30)\n",
    "\n",
    "# UÅ¾darome narÅ¡yklÄ™\n",
    "driver.quit()\n",
    "\n",
    "print(\"ğŸ”„ Apdorojimas baigtas! Duomenys iÅ¡saugoti Ä¯ imones_duomenys.csv âœ…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# CSV failo pavadinimas\n",
    "csv_failas = \"imones_duomenys.csv\"\n",
    "\n",
    "# Perskaitome jau surinktus URL iÅ¡ CSV failo\n",
    "existing_urls = set()\n",
    "try:\n",
    "    with open(csv_failas, mode=\"r\", encoding=\"utf-8\") as failas:\n",
    "        reader = csv.reader(failas)\n",
    "        next(reader)  # PraleidÅ¾iame antraÅ¡tes\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                existing_urls.add(row[0])\n",
    "except FileNotFoundError:\n",
    "    # Jei failo nÄ—ra, sukuriame jÄ¯ su antraÅ¡tÄ—mis\n",
    "    with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([\"URL\", \"Pavadinimas\", \"ApraÅ¡ymas\", \"Telefonas\", \"SvetainÄ—\", \"El. paÅ¡tas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sÄ…raÅ¡Ä… iÅ¡ failo\n",
    "with open(\"surinkti_urls.txt\", \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Filtruojame tik tuos URL, kurie dar nÄ—ra apdoroti\n",
    "neapdoroti_urls = [url for url in url_sarasas if url not in existing_urls]\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™ su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per neapdorotus URL\n",
    "for url in neapdoroti_urls:\n",
    "    print(f\"Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Palaukiame, kol puslapis uÅ¾sikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # ApraÅ¡ymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"ApraÅ¡ymas nerastas\"\n",
    "\n",
    "    # UÅ¾klausos Ä¯ svetainÄ™ su BeautifulSoup\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # SvetainÄ—\n",
    "    svetaine = \"SvetainÄ—s adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paÅ¡tas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paÅ¡tas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # IÅ¡saugome informacijÄ… Ä¯ CSV failÄ…\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"âœ… Duomenys iÅ¡saugoti: {pavadinimas}\")\n",
    "\n",
    "    # Palaukiame 5 sekundes prieÅ¡ pereinant prie kito URL\n",
    "    time.sleep(30)\n",
    "\n",
    "# UÅ¾darome narÅ¡yklÄ™\n",
    "driver.quit()\n",
    "\n",
    "print(\"ğŸ”„ Apdorojimas baigtas! Duomenys iÅ¡saugoti Ä¯ imones_duomenys.csv âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas kameval kategorija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# FailÅ³ pavadinimai\n",
    "txt_failas = \"Karneval_Kostume_entdecken.txt\"\n",
    "csv_failas = \"Karneval_Kostume_entdecken.csv\"\n",
    "\n",
    "# Perskaitome jau surinktus URL iÅ¡ CSV failo\n",
    "existing_urls = set()\n",
    "try:\n",
    "    with open(csv_failas, mode=\"r\", encoding=\"utf-8\") as failas:\n",
    "        reader = csv.reader(failas)\n",
    "        next(reader)  # PraleidÅ¾iame antraÅ¡tes\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                existing_urls.add(row[0])\n",
    "except FileNotFoundError:\n",
    "    # Jei failo nÄ—ra, sukuriame jÄ¯ su antraÅ¡tÄ—mis\n",
    "    with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([\"URL\", \"Pavadinimas\", \"ApraÅ¡ymas\", \"Telefonas\", \"SvetainÄ—\", \"El. paÅ¡tas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sÄ…raÅ¡Ä… iÅ¡ failo\n",
    "with open(txt_failas, \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Filtruojame tik tuos URL, kurie dar nÄ—ra apdoroti\n",
    "neapdoroti_urls = [url for url in url_sarasas if url not in existing_urls]\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™ su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per neapdorotus URL\n",
    "for url in neapdoroti_urls:\n",
    "    print(f\"ğŸ”¹ [INFO] Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(30)  # Palaukiame, kol puslapis uÅ¾sikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # ApraÅ¡ymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"ApraÅ¡ymas nerastas\"\n",
    "\n",
    "    # UÅ¾klausos Ä¯ svetainÄ™ su BeautifulSoup\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        print(f\"âŒ [ERROR] Nepavyko pasiekti URL: {url}\")\n",
    "        continue  # PraleidÅ¾iame Å¡Ä¯ URL\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # SvetainÄ—\n",
    "    svetaine = \"SvetainÄ—s adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paÅ¡tas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paÅ¡tas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # IÅ¡saugome informacijÄ… Ä¯ CSV failÄ…\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"âœ… [INFO] Duomenys iÅ¡saugoti: {pavadinimas}\")\n",
    "\n",
    "    # Palaukiame 30 sekundÅ¾iÅ³ prieÅ¡ pereinant prie kito URL\n",
    "    time.sleep(30)\n",
    "\n",
    "# UÅ¾darome narÅ¡yklÄ™\n",
    "driver.quit()\n",
    "\n",
    "print(\"ğŸ”„ [INFO] Apdorojimas baigtas! Duomenys iÅ¡saugoti Ä¯ Karneval_Kostume_entdecken.csv âœ…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas auto_motorarad_zubehor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# FailÅ³ pavadinimai\n",
    "txt_failas = \"Auto_Motorrad_Zubehor.txt\"\n",
    "csv_failas = \"Auto_Motorrad_Zubehor.csv\"\n",
    "\n",
    "# Perskaitome jau surinktus URL iÅ¡ CSV failo\n",
    "existing_urls = set()\n",
    "try:\n",
    "    with open(csv_failas, mode=\"r\", encoding=\"utf-8\") as failas:\n",
    "        reader = csv.reader(failas)\n",
    "        next(reader)  # PraleidÅ¾iame antraÅ¡tes\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                existing_urls.add(row[0])\n",
    "except FileNotFoundError:\n",
    "    # Jei failo nÄ—ra, sukuriame jÄ¯ su antraÅ¡tÄ—mis\n",
    "    with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([\"URL\", \"Pavadinimas\", \"ApraÅ¡ymas\", \"Telefonas\", \"SvetainÄ—\", \"El. paÅ¡tas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sÄ…raÅ¡Ä… iÅ¡ failo\n",
    "with open(txt_failas, \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Filtruojame tik tuos URL, kurie dar nÄ—ra apdoroti\n",
    "neapdoroti_urls = [url for url in url_sarasas if url not in existing_urls]\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™ su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per neapdorotus URL\n",
    "for url in neapdoroti_urls:\n",
    "    print(f\"ğŸ”¹ [INFO] Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(30)  # Palaukiame, kol puslapis uÅ¾sikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # ApraÅ¡ymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"ApraÅ¡ymas nerastas\"\n",
    "\n",
    "    # UÅ¾klausos Ä¯ svetainÄ™ su BeautifulSoup\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        print(f\"âŒ [ERROR] Nepavyko pasiekti URL: {url}\")\n",
    "        continue  # PraleidÅ¾iame Å¡Ä¯ URL\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # SvetainÄ—\n",
    "    svetaine = \"SvetainÄ—s adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paÅ¡tas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paÅ¡tas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # IÅ¡saugome informacijÄ… Ä¯ CSV failÄ…\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"âœ… [INFO] Duomenys iÅ¡saugoti: {pavadinimas}\")\n",
    "\n",
    "    # Palaukiame 30 sekundÅ¾iÅ³ prieÅ¡ pereinant prie kito URL\n",
    "    time.sleep(30)\n",
    "\n",
    "# UÅ¾darome narÅ¡yklÄ™\n",
    "driver.quit()\n",
    "\n",
    "print(\"ğŸ”„ [INFO] Apdorojimas baigtas! Duomenys iÅ¡saugoti Ä¯ Auto_Motorrad_Zubehor.csv âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drogerieartikel & Kosmetik kategorija"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# FailÅ³ pavadinimai\n",
    "txt_failas = \"Drogerieartikel_Kosmetik_entdecken.txt\"\n",
    "csv_failas = \"Drogerieartikel_Kosmetik_entdecken.csv\"\n",
    "\n",
    "# Perskaitome jau surinktus URL iÅ¡ CSV failo\n",
    "existing_urls = set()\n",
    "try:\n",
    "    with open(csv_failas, mode=\"r\", encoding=\"utf-8\") as failas:\n",
    "        reader = csv.reader(failas)\n",
    "        next(reader)  # PraleidÅ¾iame antraÅ¡tes\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                existing_urls.add(row[0])\n",
    "except FileNotFoundError:\n",
    "    # Jei failo nÄ—ra, sukuriame jÄ¯ su antraÅ¡tÄ—mis\n",
    "    with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([\"URL\", \"Pavadinimas\", \"ApraÅ¡ymas\", \"Telefonas\", \"SvetainÄ—\", \"El. paÅ¡tas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sÄ…raÅ¡Ä… iÅ¡ failo\n",
    "with open(txt_failas, \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Filtruojame tik tuos URL, kurie dar nÄ—ra apdoroti\n",
    "neapdoroti_urls = [url for url in url_sarasas if url not in existing_urls]\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™ su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per neapdorotus URL\n",
    "for url in neapdoroti_urls:\n",
    "    print(f\"ğŸ”¹ [INFO] Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(30)  # Palaukiame, kol puslapis uÅ¾sikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # ApraÅ¡ymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"ApraÅ¡ymas nerastas\"\n",
    "\n",
    "    # UÅ¾klausos Ä¯ svetainÄ™ su BeautifulSoup\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        print(f\"âŒ [ERROR] Nepavyko pasiekti URL: {url}\")\n",
    "        continue  # PraleidÅ¾iame Å¡Ä¯ URL\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # SvetainÄ—\n",
    "    svetaine = \"SvetainÄ—s adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paÅ¡tas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paÅ¡tas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # IÅ¡saugome informacijÄ… Ä¯ CSV failÄ…\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"âœ… [INFO] Duomenys iÅ¡saugoti: {pavadinimas}\")\n",
    "\n",
    "    # Palaukiame 30 sekundÅ¾iÅ³ prieÅ¡ pereinant prie kito URL\n",
    "    time.sleep(30)\n",
    "\n",
    "# UÅ¾darome narÅ¡yklÄ™\n",
    "driver.quit()\n",
    "\n",
    "print(\"ğŸ”„ [INFO] Apdorojimas baigtas! Duomenys iÅ¡saugoti Ä¯ Drogerieartikel_Kosmetik_entdecken.csv âœ…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodas kuris paima duomenis iÅ¡ Bekleidung.txt url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# FailÅ³ pavadinimai\n",
    "txt_failas = \"Bekleidung.txt\"\n",
    "csv_failas = \"Bekleidung.csv\"\n",
    "\n",
    "# Perskaitome jau surinktus URL iÅ¡ CSV failo\n",
    "existing_urls = set()\n",
    "try:\n",
    "    with open(csv_failas, mode=\"r\", encoding=\"utf-8\") as failas:\n",
    "        reader = csv.reader(failas)\n",
    "        next(reader)  # PraleidÅ¾iame antraÅ¡tes\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                existing_urls.add(row[0])\n",
    "except FileNotFoundError:\n",
    "    # Jei failo nÄ—ra, sukuriame jÄ¯ su antraÅ¡tÄ—mis\n",
    "    with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([\"URL\", \"Pavadinimas\", \"ApraÅ¡ymas\", \"Telefonas\", \"SvetainÄ—\", \"El. paÅ¡tas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sÄ…raÅ¡Ä… iÅ¡ failo\n",
    "with open(txt_failas, \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Filtruojame tik tuos URL, kurie dar nÄ—ra apdoroti\n",
    "neapdoroti_urls = [url for url in url_sarasas if url not in existing_urls]\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™ su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# Keliaujame per neapdorotus URL\n",
    "for url in neapdoroti_urls:\n",
    "    print(f\"ğŸ”¹ [INFO] Apdorojamas URL: {url}\")\n",
    "    driver.get(url)\n",
    "    time.sleep(30)  # Palaukiame, kol puslapis uÅ¾sikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = driver.find_element(By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\").text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # ApraÅ¡ymas\n",
    "    try:\n",
    "        aprasymas = driver.find_element(By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\").text\n",
    "    except Exception:\n",
    "        aprasymas = \"ApraÅ¡ymas nerastas\"\n",
    "\n",
    "    # UÅ¾klausos Ä¯ svetainÄ™ su BeautifulSoup\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        print(f\"âŒ [ERROR] Nepavyko pasiekti URL: {url}\")\n",
    "        continue  # PraleidÅ¾iame Å¡Ä¯ URL\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: x and x.startswith('+'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # SvetainÄ—\n",
    "    svetaine = \"SvetainÄ—s adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paÅ¡tas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paÅ¡tas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # IÅ¡saugome informacijÄ… Ä¯ CSV failÄ…\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"âœ… [INFO] Duomenys iÅ¡saugoti: {pavadinimas}\")\n",
    "\n",
    "    # Palaukiame 30 sekundÅ¾iÅ³ prieÅ¡ pereinant prie kito URL\n",
    "    time.sleep(30)\n",
    "\n",
    "# UÅ¾darome narÅ¡yklÄ™\n",
    "driver.quit()\n",
    "\n",
    "print(\"ğŸ”„ [INFO] Apdorojimas baigtas! Duomenys iÅ¡saugoti Ä¯ Drogerieartikel_Kosmetik_entdecken.csv âœ…\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer_Unterhaltungselektronik_Zubehor.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# FailÅ³ pavadinimai\n",
    "txt_failas = \"Consulting.txt\"\n",
    "csv_failas = \"Consulting.csv\"\n",
    "\n",
    "# Perskaitome jau surinktus URL iÅ¡ CSV failo\n",
    "existing_urls = set()\n",
    "try:\n",
    "    with open(csv_failas, mode=\"r\", encoding=\"utf-8\") as failas:\n",
    "        reader = csv.reader(failas)\n",
    "        first_line = next(reader, None)  # PraleidÅ¾iame antraÅ¡tes, jei yra\n",
    "        if first_line is not None:  # Tikriname, ar failas ne tuÅ¡Äias\n",
    "            for row in reader:\n",
    "                if row:\n",
    "                    existing_urls.add(row[0])\n",
    "except FileNotFoundError:\n",
    "    # Jei failo nÄ—ra, sukuriame jÄ¯ su antraÅ¡tÄ—mis\n",
    "    with open(csv_failas, mode=\"w\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([\"URL\", \"Pavadinimas\", \"ApraÅ¡ymas\", \"Telefonas\", \"SvetainÄ—\", \"El. paÅ¡tas\", \"Adresas\"])\n",
    "\n",
    "# Perskaitome URL sÄ…raÅ¡Ä… iÅ¡ failo\n",
    "with open(txt_failas, \"r\", encoding=\"utf-8\") as failas:\n",
    "    url_sarasas = [line.strip() for line in failas.readlines()]\n",
    "\n",
    "# Filtruojame tik tuos URL, kurie dar nÄ—ra apdoroti\n",
    "neapdoroti_urls = [url for url in url_sarasas if url not in existing_urls]\n",
    "\n",
    "# Sukuriame narÅ¡yklÄ™ su nustatymais\n",
    "nustatymai = Options()\n",
    "nustatymai.add_argument('--headless')  # Veikia fone\n",
    "nustatymai.add_argument('--disable-blink-features=AutomationControlled')  # MaÅ¾ina botÅ³ aptikimÄ…\n",
    "driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# HTTP antraÅ¡tÄ—s, kad atrodytume kaip tikras vartotojas\n",
    "headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
    "\n",
    "# Keliaujame per neapdorotus URL\n",
    "for i, url in enumerate(neapdoroti_urls):\n",
    "    print(f\"ğŸ”¹ [INFO] Apdorojamas URL: {url}\")\n",
    "    \n",
    "    # Pirmiausia tikriname, ar URL pasiekiamas\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    except requests.RequestException:\n",
    "        print(f\"âŒ [ERROR] Nepavyko pasiekti URL: {url}\")\n",
    "        continue\n",
    "\n",
    "    # Naudojame Selenium tik jei reikia\n",
    "    driver.get(url)\n",
    "    time.sleep(30)  # Palaukiame 30 sekundÅ¾iÅ³, kol puslapis uÅ¾sikraus\n",
    "\n",
    "    # Pavadinimas\n",
    "    try:\n",
    "        pavadinimas = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.sc-79691b37-3.eHuYIg span\"))\n",
    "        ).text\n",
    "    except Exception:\n",
    "        pavadinimas = \"Pavadinimas nerastas\"\n",
    "\n",
    "    # ApraÅ¡ymas\n",
    "    try:\n",
    "        aprasymas = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".companyDetails_companyDescription__rruNt span\"))\n",
    "        ).text\n",
    "    except Exception:\n",
    "        aprasymas = \"ApraÅ¡ymas nerastas\"\n",
    "\n",
    "    # Telefonas\n",
    "    phone = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=lambda x: x and x.startswith('tel:'))\n",
    "    telefonas = phone['href'].replace('tel:', '') if phone else 'Telefonas nerastas'\n",
    "\n",
    "    # SvetainÄ—\n",
    "    svetaine = \"SvetainÄ—s adresas nerastas\"\n",
    "    for link in soup.find_all('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True):\n",
    "        href = link['href']\n",
    "        if href.startswith(\"http\") or href.startswith(\"//\"):\n",
    "            svetaine = href if href.startswith(\"http\") else \"http:\" + href\n",
    "            break\n",
    "\n",
    "    # El. paÅ¡tas\n",
    "    email = soup.find('a', class_='contactInfo_companyContactDetailLink__OzJ99', href=True, string=lambda x: '@' in x)\n",
    "    el_pastas = email.string if email else 'El. paÅ¡tas nerastas'\n",
    "\n",
    "    # Adresas\n",
    "    address_blocks = soup.find_all('div', class_='contactInfo_companyContactDetail__d2tsS')\n",
    "    adresas = ' '.join([el.strip() for block in address_blocks if not block.find('a') for el in block.stripped_strings]).replace('\\xa0', ' ')\n",
    "\n",
    "    # IÅ¡saugome informacijÄ… Ä¯ CSV failÄ…\n",
    "    with open(csv_failas, mode=\"a\", newline=\"\", encoding=\"utf-8\") as failas:\n",
    "        writer = csv.writer(failas)\n",
    "        writer.writerow([url, pavadinimas, aprasymas, telefonas, svetaine, el_pastas, adresas])\n",
    "\n",
    "    print(f\"âœ… [INFO] Duomenys iÅ¡saugoti: {pavadinimas}\")\n",
    "\n",
    "    # Laukiame 30 sekundÅ¾iÅ³ prieÅ¡ kitÄ… URL\n",
    "    time.sleep(30)\n",
    "\n",
    "    # Perkrauname narÅ¡yklÄ™ kas 50 URL, kad iÅ¡vengtume atminties problemÅ³\n",
    "    if i % 50 == 0 and i != 0:\n",
    "        driver.quit()\n",
    "        driver = webdriver.Chrome(options=nustatymai)\n",
    "\n",
    "# UÅ¾darome narÅ¡yklÄ™\n",
    "driver.quit()\n",
    "\n",
    "print(\"ğŸ”„ [INFO] Apdorojimas baigtas! Duomenys iÅ¡saugoti Ä¯ Consulting.csv âœ…\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
